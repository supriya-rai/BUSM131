{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder  \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Model evaluation und visualisation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameter optimisation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(842)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/utathya/electricity-consumption\n",
    "\n",
    "energy = pd.read_csv('Electricity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26496 entries, 0 to 26495\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       26496 non-null  int64  \n",
      " 1   datetime                 26496 non-null  object \n",
      " 2   temperature              26496 non-null  float64\n",
      " 3   var1                     26496 non-null  float64\n",
      " 4   pressure                 26496 non-null  float64\n",
      " 5   windspeed                26496 non-null  float64\n",
      " 6   var2                     26496 non-null  object \n",
      " 7   electricity_consumption  26496 non-null  float64\n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "energy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    25239\n",
      "C     1040\n",
      "B      217\n",
      "Name: var2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the categorical data\n",
    "print(energy['var2'].value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data\n",
    "\n",
    "The data is a time-series of hourly energy consumptions. This usually calls for a time-series analysis, e.g. with Recurrent Neural Networks. \n",
    "Here we take a different approach and try to predict the consumption for any day, simply given the weather data. To be able to account for seasons, we include the energy consumption of the previous day to our prediction. See below for the business case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous day can be found by shifting the dataset by 24 places. \n",
    "# This creates unknown data (NaN) for the first 24 rows, which are thus deleted\n",
    "energy['consumption_previous_day'] = energy['electricity_consumption'].shift(24)\n",
    "energy = energy.dropna()\n",
    "# droping the first 24 entries did not adapt the index, so we reset it to start again from 1 \n",
    "energy = energy.reset_index()\n",
    "energy=energy.drop('index',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature ID is not useful for us\n",
    "energy=energy.drop('ID',axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract the hour of the day, which will have a significant effect on the energy consumption\n",
    "energy['hour_of_day'] = [int(date_string[11:13]) for date_string in energy['datetime']]\n",
    "energy.drop('datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and label \n",
    "labels = energy['electricity_consumption']\n",
    "energy.drop('electricity_consumption', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Train split\n",
    "We avoid further investigation of the full dataset. Instead, we split the data and work on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the category 'var2' being unevenly distributed, we chose to stratify it.\n",
    "# without knowing the details of this variable, we cannot decide for sure, if this is reasonable\n",
    "\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(\n",
    "    energy, labels.values, stratify=energy['var2'], test_size=0.2)\n",
    "\n",
    "# Reset the indices:\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "X_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data:\n",
    "\n",
    "Fill missing values for numerals\n",
    "\n",
    "Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Encoder replaces categorical features by boolean features, stating wether a certain category is true or not\n",
    "cat_feature_names = ['var2']\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_feature_names)),\n",
    "        ('cat_encoder', OrdinalEncoder()),\n",
    "    ])\n",
    "\n",
    "# We run the pipeline to check, whether it runs with no problems\n",
    "temp = cat_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attribs = X_train.columns \n",
    "num_attribs = [attribute for attribute in all_attribs if attribute not in cat_feature_names]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# We run the pipeline to check, whether it runs with no problems\n",
    "temp = num_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine both imputers and get cleaned data\n",
    "\n",
    "all_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "X_prepared = all_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21177, 7) (21177,)\n"
     ]
    }
   ],
   "source": [
    "print(X_prepared.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21177, 119)\n"
     ]
    }
   ],
   "source": [
    "# Introducing polynomial features drastically increases the dimension:\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)  # YL: Best degree\n",
    "X_poly = poly_features.fit_transform(X_prepared)\n",
    "print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r^2    0.425 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A Lasso regularisation reduces the number of nonzero coefficients,\n",
    "#  making the polynomial regression easier to interpret\n",
    "cubic_regression_lasso = Lasso(alpha=0.01, max_iter=1e5)   # YL: Best Alpha\n",
    "cubic_regression_lasso.fit(X_poly, train_labels)\n",
    "score = cubic_regression_lasso.score(X_poly, train_labels)\n",
    "print(' r^2 %8.3f \\n' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-65.  55.  -8. -52.  39.  39.   0.  10.  -6.  -5.  47. -27.  -5.  39.\n",
      "  -5.   9. -18.  32.  -1.   0.  -2.   1.  -1.  -1.   5.  11. -24.  -6.\n",
      "  93.  -7.   6. -38.  -6.  20.   0.  -2.  -7.   3.  -9.   8.  -2.  -0.\n",
      "  28. -12.  16. -14.   0.  13.  -4.  -1.   0.   2.  -4.  -4.  13.   2.\n",
      "  -2.   2.   1.  12.  -8.   4. -12.  -6.   7.   6.   2.  -2.   0.  -1.\n",
      "  -4.   2.   1.  -0.   4. -16.  -0.  16.  -2.  -0.  -6.   2.   1.  -8.\n",
      "  -2.  -1.  -2.   3.   1.  -0.   5.   1.  -2.   1.   0.   3.  -1.   1.\n",
      "   0.  -1.   2.   1.  10.   2.  -2.  21.   3.  -1. -57.   0.  -2.   3.\n",
      "   0.  -1.  12. -13.   2. -14.  -7.]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0, suppress=True)\n",
    "print(cubic_regression_lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n",
       "                ('lasso', Lasso(alpha=0.01, max_iter=20000.0))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-Create and the optimal model  # YL\n",
    "def PolynomialRegression(degree=3, alpha=.01, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), Lasso(alpha, max_iter=2e4, **kwargs))\n",
    "\n",
    "cubic_regression_lasso = PolynomialRegression()\n",
    "cubic_regression_lasso.fit(X_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using the pipeline defined earlier\n",
    "X_test_prepared = all_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the test data:\n",
    "prediction_test = cubic_regression_lasso.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r^2    0.437 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = cubic_regression_lasso.score(X_test_prepared, test_labels)\n",
    "print(' r^2 %8.3f \\n' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6679"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mean_squared_error(test_labels, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to trivial prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often the error value does not say much about the quality of the model, as it highly depends on the data. One way to assess the quality is by comparing to a trivial prediction. This can be the mean value of a subclass or for time-series a data point of the past. Here, we use the electricity consumption of the previous day as the trivial prediction. \n",
    "\n",
    "Note that this is only useful, if we evaluate the model on a daily basis. If we require a prediction for the next year, the consumption of the previous day is clearly not available as an estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_previous = X_test['consumption_previous_day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14670"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mean_squared_error(test_labels, labels_previous))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case\n",
    "\n",
    "A good prediction of the electricity consumption is essential for the costing of electricity generation. The costing covers capital cost, operational cost and ramping up cost and time with considerbale differences between technologies and fuels. For example, Coal-fired combustion turbine capital cost is typically 1,00 USD/kW (US dollar per thousand kilo-watt), operation cost is typically 0.04 USD/kWh and typical ramp-up time is an hour. These costs for Natural gas combustion turbine are 800 USD/kW, 0.10 USD/kWh (US dollar per thousand kilo-watt hour) and ramp-up of ten minutes. A typical operational strategy is to operate coal (or oil) turbines to provide the required normal capacity and to use natural gas turbines when there are short-term hikes in required capacity. It usually also makes sense to have large coal or oil turbines where operational costs are low and small gas turnines that can be ramped-up and turned off quickly as more or less un-predicted capacity is required. \n",
    "\n",
    "For a detailed course of the economics of electricity see https://www.e-education.psu.edu/ebf483/node/517) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:\n",
    "\n",
    "When the  prediction of the required capacity is higher than the coal-turbines capacity, the plant will put into operation gas-turbines with higher operational cost. One possible issue is then how many times we predict a higher capacity that does not materialize. Each such occurance will cost the ramp-up of a turbine. Below we count the time this mis-prediction happens (each is per hour) and compare the Laso cubic model to the trivial model of the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumPositiveErrorsOccurance(actual,predicted):\n",
    "    d1= actual-predicted\n",
    "    d2=d1>0\n",
    "    return d2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error versus actual      :  2301\n",
      " Error versus previous day:  2920\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0, suppress=True)\n",
    "print(' Error versus actual      : ',round(SumPositiveErrorsOccurance(test_labels, prediction_test)) )\n",
    "print(' Error versus previous day: ',round(SumPositiveErrorsOccurance(test_labels, labels_previous)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:\n",
    "\n",
    "The plant tries to use the minimal number of coal-turbines and no gas-turnines which are costlier. When the  prediction of the required capacity is higher than the coal-turbines capacity, the plant will put into operation gas-turbines with higher operational cost. One possible issue is then how many times we predict a lower capacity that does not materialize. Each such occurance will cost the ramp-up of a turbine. Below we count the times this mis-prediction happens (each is per hour) and compare the Laso cubic model to the trivial model of the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumNegativeErrorsOccurance(actual,predicted):\n",
    "    d1= actual-predicted\n",
    "    d2=d1<0\n",
    "    return abs(d2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error versus actual      :  2994\n",
      " Error versus previous day:  2287\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0, suppress=True)\n",
    "print(' Error versus actual      : ',round(SumNegativeErrorsOccurance(test_labels, prediction_test)) )\n",
    "print(' Error versus previous day: ',round(SumNegativeErrorsOccurance(test_labels, labels_previous)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3:\n",
    "\n",
    "With the positive and negative occurance errors, it is not clear which model is better. A detailed modelling of the power plant would help -- what is the capacity of the oil and gas turbines, what is the cost of ramp-up, etc. However, operational managers have also their rule of thumb, the negative errors (capacity is missing) are much worse than positive errors (active capacity is idle). The reason is that negative errors require them to buy capacity from other providers and to quickly turn on additional turbines, while positive errors cost some money, but they are not critical to the operation. The following simple code models this rule of thumb: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301 2920 2994 2287\n",
      "-619 707\n",
      "88\n",
      "6451\n"
     ]
    }
   ],
   "source": [
    "factor=10\n",
    "#\n",
    "p_model=SumPositiveErrorsOccurance(test_labels, prediction_test)\n",
    "p_guess=SumPositiveErrorsOccurance(test_labels, labels_previous)\n",
    "#\n",
    "n_model=SumNegativeErrorsOccurance(test_labels, prediction_test)\n",
    "n_guess=SumNegativeErrorsOccurance(test_labels, labels_previous)\n",
    "#\n",
    "print(  p_model, p_guess, n_model,n_guess  )\n",
    "print(  (p_model-p_guess) , (n_model-n_guess)  )\n",
    "print(  (p_model-p_guess) + (n_model-n_guess)  )\n",
    "print(  (p_model-p_guess) + factor*(n_model-n_guess)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_labels==prediction_test))\n",
    "print(sum(test_labels==labels_previous))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4:\n",
    "\n",
    "When gas turbines are required, their operation is costlier, so now we check how the models perform as errors require costlier turbines. This issue is similar to Task 2, but now we count the actual gas-turnine capacity not just an occurance of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumNegativeErrors(actual,predicted):\n",
    "    d1= actual-predicted\n",
    "    d2=d1[d1<0]\n",
    "    return abs(d2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error versus actual      :  148230\n",
      " Error versus previous day:  213057\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=0, suppress=True)\n",
    "print(' Error versus actual      : ',round(SumNegativeErrors(test_labels, prediction_test)) )\n",
    "print(' Error versus previous day: ',round(SumNegativeErrors(test_labels, labels_previous)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
